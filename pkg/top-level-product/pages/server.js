// server.js
const express = require('express');
const cors = require('cors');
const { spawn } = require('child_process');

// --- Polyfill fetch cho Node c≈© (∆∞u ti√™n undici) ---
try {
    const { fetch, Headers, Request, Response } = require('undici');
    Object.assign(globalThis, { fetch, Headers, Request, Response });
} catch {
    const fetch = require('node-fetch');
    Object.assign(globalThis, {
        fetch,
        Headers: fetch.Headers,
        Request: fetch.Request,
        Response: fetch.Response,
    });
}

// --- OpenAI SDK (tr·ªè Ollama) ---
const OpenAI = require('openai');

// ====== C·∫§U H√åNH ======
const OLLAMA_BASE = process.env.OLLAMA_BASE || 'http://192.168.10.32:11434/v1';
const MODEL_NAME = process.env.MODEL_NAME || 'gpt-oss:20b'; // model trong Ollama
const MCP_BASE = process.env.MCP_BASE || 'http://192.168.10.18:3000'; // http://host:port

// (n·∫øu MCP c·∫ßn header nh∆∞ Authorization th√¨ th√™m ·ªü ƒë√¢y)
const MCP_HEADERS = {
    'content-type': 'application/json'
    // 'authorization': `Bearer ${process.env.MCP_TOKEN}`
};

// ====== OPENAI CLIENT (Ollama-compatible) ======
const openai = new OpenAI({
    baseURL: OLLAMA_BASE,
    apiKey: 'ollama', // placeholder
});

// ====== MCP CLIENT (HTTP + JSON-RPC) ======
async function mcpCall(method, params = {}, id = Date.now()) {
    const r = await fetch(`${MCP_BASE}/messages`, {
        method: 'POST',
        headers: MCP_HEADERS,
        body: JSON.stringify({
            jsonrpc: '2.0',
            id,
            method,
            params,
        }),
    });
    if (!r.ok) throw new Error(`MCP HTTP ${r.status}`);
    const data = await r.json();
    if (data.error) throw new Error(`MCP error: ${data.error.message || 'unknown'}`);
    return data.result;
}

// (Tu·ª≥ server: m·ªü SSE ƒë·ªÉ gi·ªØ session/nh·∫≠n notify; kh√¥ng b·∫Øt bu·ªôc n·∫øu ch·ªâ g·ªçi l·∫ª)
async function mcpInitialize() {
    // G·ª≠i initialize ƒë·ªÉ server bi·∫øt client-info + capabilities
    return mcpCall('initialize', {
        protocolVersion: '2025-06-18',
        capabilities: { tools: {}, resources: {}, prompts: {} },
        clientInfo: { name: 'ollama-mcp-client', version: '0.1.0' },
    });
}

async function mcpListTools() {
    const res = await mcpCall('tools/list', {});
    // res.tools: [{name, description, inputSchema, outputSchema?}, ...]
    return res.tools || [];
}

async function mcpToolsCall(name, args) {
    // Chu·∫©n MCP: tools/call v·ªõi { name, arguments }
    const res = await mcpCall('tools/call', { name, arguments: args });
    // K·∫øt qu·∫£ th∆∞·ªùng c√≥ { content: string | object, ... }
    return res;
}

// ====== CHUY·ªÇN schema MCP -> tools OpenAI-compatible (Ollama) ======
function mapMcpToolsToOpenAITools(mcpTools) {
    // Ollama/OpenAI: { type:"function", function:{ name, description, parameters } }
    return mcpTools.map(t => ({
        type: 'function',
        function: {
            name: t.name,
            description: t.description || `MCP tool: ${t.name}`,
            parameters: t.inputSchema || { type: 'object', properties: {} },
        }
    }));
}

// ====== V√íNG L·∫∂P TOOL-CALLING (thu·∫ßn Ollama) ======
async function runToolCallingWithOllama({ userMessages, tools }) {
    // userMessages: [{role, content}...]
    // tools: m·∫£ng OpenAI-compatible (t·ª´ MCP)
    const messages = userMessages.slice();
    messages.push({
        role: 'system',
        content:
            'Khi c·∫ßn thao t√°c Kubernetes, h√£y g·ªçi function th√≠ch h·ª£p (ƒë·ª´ng ƒëo√°n). ' +
            'Tr·∫£ l·ªùi ng·∫Øn g·ªçn, n·∫øu g·ªçi tool th√¨ ch·ªù k·∫øt qu·∫£ tool.'
    });

    // Gi·ªõi h·∫°n v√≤ng l·∫∑p tool-calls
    let guard = 0;

    while (guard++ < 6) {
        const completion = await openai.chat.completions.create({
            model: MODEL_NAME,
            messages,
            tools,
            tool_choice: 'auto' // n·∫øu model hay "g·ªçi b·ª´a", b·∫°n c√≥ th·ªÉ b·ªè d√≤ng n√†y
        });

        const choice = completion.choices?.[0];
        const msg = choice?.message || {};
        const toolCalls = msg.tool_calls || msg.toolCalls || []; // tu·ª≥ model

        // N·∫øu model mu·ªën g·ªçi tool
        if (Array.isArray(toolCalls) && toolCalls.length > 0) {
            // Ghi l·∫°i assistant tool_calls ƒë·ªÉ duy tr√¨ "trace"
            messages.push({ role: 'assistant', tool_calls: toolCalls });

            // Th·ª±c thi l·∫ßn l∆∞·ª£t t·ª´ng tool_call
            for (const tc of toolCalls) {
                const { id, function: fn } = tc;
                const name = fn?.name;
                let args = {};
                try {
                    args = fn?.arguments ? JSON.parse(fn.arguments) : {};
                } catch {
                    // n·∫øu parse fail, ƒë·ªÉ args = {}
                }

                // G·ªçi qua MCP
                let toolOutput = '';
                try {
                    const mcpRes = await mcpToolsCall(name, args);
                    // Chu·∫©n ho√° output th√†nh string ng·∫Øn g·ªçn
                    if (mcpRes?.content !== undefined) {
                        toolOutput =
                            typeof mcpRes.content === 'string'
                                ? mcpRes.content
                                : JSON.stringify(mcpRes.content);
                    } else {
                        toolOutput = JSON.stringify(mcpRes);
                    }
                } catch (e) {
                    toolOutput = `ERROR from MCP: ${e.message}`;
                }

                // ƒê·∫©y k·∫øt qu·∫£ tool v·ªÅ cho model
                messages.push({
                    role: 'tool',
                    tool_call_id: id, // r·∫•t quan tr·ªçng ƒë·ªÉ model "li√™n k·∫øt" k·∫øt qu·∫£
                    content: truncate(toolOutput, 48 * 1024)
                });
            }

            // quay l·∫°i v√≤ng l·∫∑p ƒë·ªÉ model t·ªïng h·ª£p k·∫øt qu·∫£
            continue;
        }

        // Kh√¥ng c√≤n tool_calls ‚Üí ƒë√¢y l√† c√¢u tr·∫£ l·ªùi cu·ªëi
        const finalText = msg.content || '(no content)';
        return { text: finalText, trace: messages };
    }

    return { text: '‚ö†Ô∏è D·ª´ng do qu√° nhi·ªÅu v√≤ng tool-calling', trace: messages };
}

const truncate = (s, n) => (s && s.length > n ? s.slice(0, n) + '\n...[truncated]...' : s || '');

// ====== EXPRESS APP ======
const app = express();
app.use(cors());
app.use(express.json());

// Cache tools (t·ª´ MCP) ƒë·ªÉ kh·ªèi g·ªçi l·∫°i m·ªói request
let OPENAI_COMPAT_TOOLS = [];

app.post('/api/chat', async (req, res) => {
    try {
        const userMessages = (req.body.messages || []).map(m => ({
            role: m.role,
            content: m.content
        }));

        // 1) initialize MCP (1 l·∫ßn m·ªói process; ·ªü ƒë√¢y g·ªçi "best effort")
        try { await mcpInitialize(); } catch (e) { console.warn('MCP init warn:', e.message); }

        // 2) l·∫•y tools t·ª´ MCP n·∫øu cache tr·ªëng (ho·∫∑c b·∫°n t·ª± l√™n l·ªãch refresh)
        if (OPENAI_COMPAT_TOOLS.length === 0) {
            const mcpTools = await mcpListTools();
            OPENAI_COMPAT_TOOLS = mapMcpToolsToOpenAITools(mcpTools);
            console.log('üîß Loaded tools from MCP:', mcpTools.map(t => t.name));
        }

        // 3) ch·∫°y v√≤ng l·∫∑p tool-calling v·ªõi Ollama
        const result = await runToolCallingWithOllama({
            userMessages,
            tools: OPENAI_COMPAT_TOOLS
        });

        return res.json({
            message: { content: result.text }
            // , trace: result.trace   // b·∫≠t n·∫øu mu·ªën debug
        });
    } catch (err) {
        console.error('Chat error:', err?.message || err);
        return res.status(502).json({ message: { content: '‚ùå L·ªói x·ª≠ l√Ω y√™u c·∫ßu' } });
    }
});

// health
app.get('/health', (_req, res) => res.json({ ok: true }));

app.listen(8055, '0.0.0.0', () => {
    console.log('‚úÖ API ch·∫°y: http://0.0.0.0:8055');
    console.log('ü§ñ Ollama baseURL:', OLLAMA_BASE, ' | MODEL:', MODEL_NAME);
    console.log('üîå MCP base:', MCP_BASE, ' (/messages, /sse)');
});
